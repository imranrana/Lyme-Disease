{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPrsLBDLZDLK"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, Sequential\n",
    "from keras.layers import Input, Conv2D, SeparableConv2D,MaxPooling2D, Dense, Flatten, Reshape, Dropout, BatchNormalization, Activation, GlobalAveragePooling2D, GlobalMaxPool2D\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR               = 0.01 \n",
    "WEIGHT_DECAY          = 1e-5\n",
    "CLIP_THRESHOLD        = 1.0\n",
    "SEED=42\n",
    "MAX_EPOCH = 250\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
    "    ):\n",
    "        super(WarmUpCosine, self).__init__()\n",
    "\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.pi = tf.constant(np.pi)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "        'learning_rate_base': self.learning_rate_base,\n",
    "        'total_steps': self.total_steps,\n",
    "        'warmup_learning_rate': self.warmup_learning_rate,\n",
    "        'warmup_steps': self.warmup_steps,\n",
    "         }\n",
    "        return config\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self.total_steps < self.warmup_steps:\n",
    "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
    "        learning_rate = (\n",
    "            0.5\n",
    "            * self.learning_rate_base\n",
    "            * (\n",
    "                1\n",
    "                + tf.cos(\n",
    "                    self.pi\n",
    "                    * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
    "                    / float(self.total_steps - self.warmup_steps)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if self.warmup_steps > 0:\n",
    "            if self.learning_rate_base < self.warmup_learning_rate:\n",
    "                raise ValueError(\n",
    "                    \"Learning_rate_base must be larger or equal to \"\n",
    "                    \"warmup_learning_rate.\"\n",
    "                )\n",
    "            slope = (\n",
    "                self.learning_rate_base - self.warmup_learning_rate\n",
    "            ) / self.warmup_steps\n",
    "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps, warmup_rate, learning_rate\n",
    "            )\n",
    "        return tf.where(\n",
    "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKmema3UnTIF"
   },
   "outputs": [],
   "source": [
    "kaiming_normal = keras.initializers.VarianceScaling(scale=2.0, mode='fan_out', distribution='untruncated_normal')\n",
    "\n",
    "def conv3x3(x, out_planes, stride=1, dilation_rate=1, name=None):\n",
    "    x = layers.ZeroPadding2D(padding=1, name=f'{name}_pad')(x)\n",
    "    return layers.Conv2D(filters=out_planes, kernel_size=3, strides=stride, use_bias=False, kernel_initializer=kaiming_normal, name=name)(x)\n",
    " \n",
    "def eca_block(inputs, b=1, gama=2):\n",
    "    \n",
    "    #  Enter the number of channels of the characteristic graph \n",
    "    in_channel = inputs.shape[-1]\n",
    "    \n",
    "    #  Calculate the size of adaptive convolution kernel according to the formula \n",
    "    kernel_size = int(abs((math.log(in_channel, 2) + b) / gama))\n",
    "    \n",
    "    #  If the convolution kernel size is even , Just use it \n",
    "    if kernel_size % 2:\n",
    "        kernel_size = kernel_size\n",
    "    \n",
    "    #  If the convolution kernel size is odd, it becomes even \n",
    "    else:\n",
    "        kernel_size = kernel_size + 1\n",
    "    \n",
    "    # [h,w,c]==>[None,c]  Global average pooling \n",
    "    x = layers.GlobalAveragePooling2D()(inputs)\n",
    "    \n",
    "    # [None,c]==>[c,1]\n",
    "    x = layers.Reshape(target_shape=(in_channel, 1))(x)\n",
    "    \n",
    "    # [c,1]==>[c,1]\n",
    "    x = layers.Conv1D(filters=1, kernel_size=kernel_size, padding='same', use_bias=False)(x)\n",
    "    \n",
    "    # sigmoid Activate \n",
    "    x = tf.nn.sigmoid(x)\n",
    "    \n",
    "    # [c,1]==>[1,1,c]\n",
    "    x = layers.Reshape((1,1,in_channel))(x)\n",
    "    \n",
    "    #  The result is multiplied by the input \n",
    "    outputs = layers.multiply([inputs, x])\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "\n",
    "\n",
    "def basic_block(x, planes, stride=1, downsample=None, name=None):\n",
    "    identity = x\n",
    "\n",
    "    out = layers.Conv2D(filters=planes, kernel_size=3, strides=1, padding='same',use_bias=False, kernel_initializer=kaiming_normal, name = f'{name}.conv1')(x)\n",
    "    out = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.bn1')(out)\n",
    "    out = tf.keras.activations.swish(out)\n",
    "    \n",
    "    out = layers.Conv2D(filters=planes, kernel_size=3, strides=stride, padding='same',use_bias=False, kernel_initializer=kaiming_normal, name = f'{name}.conv2')(out)\n",
    "    out = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.bn2')(out)\n",
    "    \n",
    "    #Channel Attention - ECA\n",
    "    out = eca_block(out)\n",
    "\n",
    "    if downsample is not None:\n",
    "        for layer in downsample:\n",
    "            identity = layer(identity)\n",
    "    \n",
    "    #skip connection\n",
    "    out = layers.Add(name=f'{name}.add')([identity, out])\n",
    "    out = tf.keras.activations.swish(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "def make_layer(x, planes, blocks, stride=1, name=None):\n",
    "    downsample = None\n",
    "    inplanes = x.shape[3]\n",
    "    if stride != 1 or inplanes != planes:\n",
    "        downsample = [\n",
    "            layers.Conv2D(filters=planes, kernel_size=1, strides=stride, use_bias=False, kernel_initializer=kaiming_normal, name=f'{name}.0.downsample.0'),\n",
    "            layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.0.downsample.1'),\n",
    "        ]\n",
    "\n",
    "    x = basic_block(x, planes, stride, downsample, name=f'{name}.0')\n",
    "    for i in range(1, blocks):\n",
    "        x = basic_block(x, planes, name=f'{name}.{i}')\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def custom(x, blocks_per_layer, num_classes=2):\n",
    "    x = layers.Rescaling(1. / 255.)(x)\n",
    "    x = layers.ZeroPadding2D(padding=3, name='conv1_pad')(x)\n",
    "    x = layers.Conv2D(filters=64, kernel_size=7, strides=2, use_bias=False, kernel_initializer=kaiming_normal, name='conv1')(x)\n",
    "    x = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='bn1')(x)\n",
    "    x = tf.keras.activations.swish(x)\n",
    "    x = layers.ZeroPadding2D(padding=1, name='maxpool_pad')(x)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, name='maxpool')(x)\n",
    "\n",
    "    x = make_layer(x, 64, blocks_per_layer[0], name='layer1')\n",
    "    x = make_layer(x, 128, blocks_per_layer[1], stride=2, name='layer2')\n",
    "    x = make_layer(x, 256, blocks_per_layer[2], stride=2, name='layer3')\n",
    "    x = make_layer(x, 512, blocks_per_layer[3], stride=2, name='layer4')\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D(name='avgpool')(x)\n",
    "    initializer = keras.initializers.RandomUniform(-1.0 / math.sqrt(512), 1.0 / math.sqrt(512))\n",
    "    x = layers.Dense(units=num_classes,activation='softmax', kernel_initializer=initializer, bias_initializer=initializer, name='fc')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def CustomArchi(x):\n",
    "    return custom(x, [2, 2, 2, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "outputs = CustomArchi(inputs)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECTURE = 'CustomArchitecture'\n",
    "\n",
    "BASE_DIR = '/tf/DAPPEM/multi_fold_data_V2'\n",
    "MODEL_SAVE_DIRECTORY_PAR =  '/tf/DAPPEM/saved-models-dv2'\n",
    "if not os.path.exists(MODEL_SAVE_DIRECTORY_PAR):\n",
    "    os.mkdir(MODEL_SAVE_DIRECTORY_PAR)\n",
    "    print(MODEL_SAVE_DIRECTORY_PAR)\n",
    "MODEL_SAVE_DIRECTORY =  os.path.join(MODEL_SAVE_DIRECTORY_PAR, ARCHITECTURE)\n",
    "\n",
    "if os.path.exists(MODEL_SAVE_DIRECTORY):\n",
    "    shutil.rmtree(MODEL_SAVE_DIRECTORY, ignore_errors=True)\n",
    "os.mkdir(MODEL_SAVE_DIRECTORY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    fold_name =  'fold'+str(i)\n",
    "    print(fold_name,'........')\n",
    "    fold_dir = os.path.join(BASE_DIR, fold_name)  \n",
    "    train_dir = os.path.join(fold_dir, 'train_dir_aug')\n",
    "    test_dir = os.path.join(BASE_DIR, 'test_dir')\n",
    "    validation_dir = os.path.join(fold_dir, 'val_dir')\n",
    "    \n",
    "    train_dataset = image_dataset_from_directory(train_dir,\n",
    "                                                 shuffle=True,\n",
    "                                                 batch_size=BATCH_SIZE,\n",
    "                                                 image_size=IMG_SIZE)\n",
    "\n",
    "    validation_dataset = image_dataset_from_directory(validation_dir,\n",
    "                                                      shuffle=True,\n",
    "                                                      batch_size=BATCH_SIZE,\n",
    "                                                      image_size=IMG_SIZE)\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    inputs = keras.Input(shape=(224, 224, 3))\n",
    "    outputs = resnet18(inputs)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    TOTAL_STEPS = len(train_dataset)* MAX_EPOCH\n",
    "    warmup_steps = int(0.05*TOTAL_STEPS)\n",
    "\n",
    "    scheduled_lrs = WarmUpCosine(learning_rate_base=INIT_LR,\n",
    "                                 total_steps=TOTAL_STEPS,\n",
    "                                 warmup_learning_rate=0.0,\n",
    "                                 warmup_steps=warmup_steps)\n",
    "\n",
    "    model.compile(optimizer=tfa.optimizers.AdamW(weight_decay=WEIGHT_DECAY, \n",
    "                                learning_rate=scheduled_lrs,\n",
    "                                clipnorm=CLIP_THRESHOLD),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics='accuracy')\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=20)\n",
    "    \n",
    "    model_name = ARCHITECTURE + fold_name + '.h5'\n",
    "    model_save_path = os.path.join(MODEL_SAVE_DIRECTORY, model_name)\n",
    "    \n",
    "    mc = ModelCheckpoint(model_save_path, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "    csv_logger = CSVLogger(os.path.join(MODEL_SAVE_DIRECTORY,\"model_history_log_fold\"+str(i)+\".csv\"), append=True)\n",
    "    history_bef_finetune = model.fit(train_dataset, epochs=MAX_EPOCH, validation_data=validation_dataset, callbacks=[mc, csv_logger])\n",
    "    pyplot.plot(history_bef_finetune.history['loss'], label='train')\n",
    "    pyplot.plot(history_bef_finetune.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "sys.path.insert(1, 'utils')\n",
    "from summarizeModelALLMetricPercPMSMCUSTOM import summarizeModel\n",
    "summarizeModel(IMG_SIZE = (224, 224),\n",
    "                  ARCHITECTURE = 'CustomArchitecture')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Modified CBAM RESNET.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
